# -*- coding: utf-8 -*-
"""Task1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NYfc91RNnQ_7Tqx9cMYteLTjVOahDVAm
"""

!pip install transformers datasets
!pip install accelerate

from transformers import (
    GPT2Tokenizer,
    GPT2LMHeadModel,
    TextDataset,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)
DataCollatorForLanguageModeling,
Trainer,
TrainingArguments

model_name = "gpt2"

tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))

from google.colab import files
uploaded = files.upload()

def load_dataset(file_path, tokenizer):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=128
    )

train_dataset = load_dataset("custom_data.txt", tokenizer)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)

# Check if the file exists and has content
import os
if not os.path.exists("custom_data.txt") or os.stat("custom_data.txt").st_size == 0:
    print("Error: custom_data.txt not found or is empty.")
    # You might want to add code here to handle the missing/empty file,
    # like prompting the user to upload the file or creating a dummy file.
else:
    def load_dataset(file_path, tokenizer):
        return TextDataset(
            tokenizer=tokenizer,
            file_path=file_path,
            block_size=128
        )

    train_dataset = load_dataset("custom_data.txt", tokenizer)

    # Add a check for the dataset size
    if len(train_dataset) == 0:
        print("Error: The loaded dataset is empty. Please check custom_data.txt content.")
    else:
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer, mlm=False,
        )

        training_args = TrainingArguments(
            output_dir="./gpt2-finetuned",
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=2,
            save_steps=500,
            save_total_limit=2,
            logging_dir='./logs',
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
        )

        trainer.train()

trainer.save_model("./gpt2-finetuned")
tokenizer.save_pretrained("./gpt2-finetuned")

from transformers import pipeline

generator = pipeline('text-generation', model="./gpt2-finetuned", tokenizer="./gpt2-finetuned")
prompt = "Once upon a time"
results = generator(prompt, max_length=100, num_return_sequences=1)

print(results[0]['generated_text'])