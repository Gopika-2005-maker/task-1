# -*- coding: utf-8 -*-
"""Task3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ACOnyeYVXw3TIh5jZQiC-_FI2X9Sd6nB
"""

import random
from collections import defaultdict

class MarkovTextGenerator:
    def _init_(self, order):
        self.order = order
        self.markov_map = defaultdict(list)

    def train(self, text):
        if not text:
            return

        # Pad the text to handle the beginning
        text = " " * self.order + text

        for i in range(len(text) - self.order):
            current_state = text[i : i + self.order]
            next_char = text[i + self.order]
            self.markov_map[current_state].append(next_char)

    def generate(self, length=100, start_state=None):
        if not self.markov_map:
            return "Model not trained."

        if start_state is None:
            start_state = random.choice(list(self.markov_map.keys()))
        elif len(start_state) != self.order:
            return f"Starting state must be of length {self.order}."
        elif start_state not in self.markov_map:
            return "Starting state not found in the trained data."

        generated_text = start_state
        current_state = start_state

        for _ in range(length - self.order):
            possible_next = self.markov_map.get(current_state)
            if not possible_next:
                break  # Stop if no next character is found for the current state

            next_char = random.choice(possible_next)
            generated_text += next_char
            current_state = generated_text[-self.order :]

        return generated_text

# Corrected the variable name to __name__
if __name__ == "__main__":
    # Example usage with character-level Markov chain of order 2
    text_corpus = "the quick brown fox jumps over the lazy fox."

    # Train the model with order 1 (predict next character based on the previous one)
    order_1_generator = MarkovTextGenerator(order=1)
    order_1_generator.train(text_corpus)
    generated_text_order_1 = order_1_generator.generate(length=50, start_state="t")
    print("Generated text (order 1):", generated_text_order_1)

    # Train the model with order 2 (predict next character based on the previous two)
    order_2_generator = MarkovTextGenerator(order=2)
    order_2_generator.train(text_corpus)
    generated_text_order_2 = order_2_generator.generate(length=50, start_state="th")
    print("Generated text (order 2):", generated_text_order_2)

    # Example with a different text and order 3
    long_text = "to be or not to be that is the question whether tis nobler in the mind to suffer the slings and arrows of outrageous fortune or to take arms against a sea of troubles and by opposing end them"
    order_3_generator = MarkovTextGenerator(order=3)
    order_3_generator.train(long_text)
    generated_text_order_3 = order_3_generator.generate(length=80, start_state="to ")
    print("Generated text (order 3):", generated_text_order_3)

import random
from collections import defaultdict

class MarkovTextGenerator:
    # Changed _init_ to __init__
    def __init__(self, order):
        self.order = order
        self.markov_map = defaultdict(list)

    def train(self, text):
        if not text:
            return

        # Pad the text to handle the beginning
        text = " " * self.order + text

        for i in range(len(text) - self.order):
            current_state = text[i : i + self.order]
            next_char = text[i + self.order]
            self.markov_map[current_state].append(next_char)

    def generate(self, length=100, start_state=None):
        if not self.markov_map:
            return "Model not trained."

        if start_state is None:
            start_state = random.choice(list(self.markov_map.keys()))
        elif len(start_state) != self.order:
            return f"Starting state must be of length {self.order}."
        elif start_state not in self.markov_map:
            return "Starting state not found in the trained data."

        generated_text = start_state
        current_state = start_state

        for _ in range(length - self.order):
            possible_next = self.markov_map.get(current_state)
            if not possible_next:
                break  # Stop if no next character is found for the current state

            next_char = random.choice(possible_next)
            generated_text += next_char
            current_state = generated_text[-self.order :]

        return generated_text

# Corrected the variable name to __name__
if __name__ == "__main__":
    # Example usage with character-level Markov chain of order 2
    text_corpus = "the quick brown fox jumps over the lazy fox."

    # Train the model with order 1 (predict next character based on the previous one)
    order_1_generator = MarkovTextGenerator(order=1)
    order_1_generator.train(text_corpus)
    generated_text_order_1 = order_1_generator.generate(length=50, start_state="t")
    print("Generated text (order 1):", generated_text_order_1)

    # Train the model with order 2 (predict next character based on the previous two)
    order_2_generator = MarkovTextGenerator(order=2)
    order_2_generator.train(text_corpus)
    generated_text_order_2 = order_2_generator.generate(length=50, start_state="th")
    print("Generated text (order 2):", generated_text_order_2)

    # Example with a different text and order 3
    long_text = "to be or not to be that is the question whether tis nobler in the mind to suffer the slings and arrows of outrageous fortune or to take arms against a sea of troubles and by opposing end them"
    order_3_generator = MarkovTextGenerator(order=3)
    order_3_generator.train(long_text)
    generated_text_order_3 = order_3_generator.generate(length=80, start_state="to ")
    print("Generated text (order 3):", generated_text_order_3)